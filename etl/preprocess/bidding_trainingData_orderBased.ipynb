{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化变量（运行模式、日期、日期区间天数等）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODE -> False\n",
      "base_dt 20170423 base_start 1492876800 base_end 1492963199\n",
      "SIZE: 14 range: DatetimeIndex(['2017-04-10', '2017-04-11', '2017-04-12', '2017-04-13',\n",
      "               '2017-04-14', '2017-04-15', '2017-04-16', '2017-04-17',\n",
      "               '2017-04-18', '2017-04-19', '2017-04-20', '2017-04-21',\n",
      "               '2017-04-22', '2017-04-23'],\n",
      "              dtype='datetime64[ns]', freq='D')\n",
      "20170410,20170411,20170412,20170413,20170414,20170415,20170416,20170417,20170418,20170419,20170420,20170421,20170422,20170423\n",
      "DATES[0] 2017-04-10 00:00:00 DATES[-1] 2017-04-23 00:00:00 dates_start 1491753600 dates_end 1492963199\n",
      "city: bj\n",
      "order dump: /user/lujin/order_20170423_14days.parquet\n",
      "dispatch info dump: /user/lujin/dispatch_info_20170423_14days.parquet\n",
      "dispatch merged dump: /user/lujin/dispatch_20170423_14days.parquet\n",
      "bidding merged dump: /user/lujin/bidding_20170423_14days.parquet\n",
      "order+dispatch dump: /user/lujin/order_all_in_one_20170423_14days.parquet\n",
      "order_merged + supply dump: /user/lujin/order_all_in_one_supply_20170423_14days.parquet\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import arrow\n",
    "\n",
    "# \n",
    "#####################################################################################\n",
    "# debug 模式会减少数据量\n",
    "DEBUG = False\n",
    "# 计算的截至日期\n",
    "base_dt = \"20170423\"\n",
    "# 截至日期向前推多少天\n",
    "days = 14\n",
    "# 计算过程中间临时文件存放的hdfs目录位置（文件名按城市自动拼接）\n",
    "hdfs_dump_base_path = \"/user/lujin\"\n",
    "# spark的并行度\n",
    "PARALLEL = 512\n",
    "#####################################################################################\n",
    "\n",
    "print \"MODE ->\", DEBUG\n",
    "\n",
    "base_date = arrow.get(base_dt, \"YYYYMMDD\")\n",
    "offset = -days + 1\n",
    "base_start = arrow.get(base_dt, \"YYYYMMDD\").to(\"Asia/Shanghai\").floor('day').timestamp\n",
    "base_end = arrow.get(base_dt, \"YYYYMMDD\").to(\"Asia/Shanghai\").ceil('day').timestamp\n",
    "print \"base_dt\", base_dt, \"base_start\", base_start, \"base_end\", base_end\n",
    "\n",
    "\n",
    "DATES = pd.date_range(base_date.replace(days=offset).format(\"YYYY-MM-DD\"), base_date.format(\"YYYY-MM-DD\"))\n",
    "DATES_STR = ','.join(map(lambda t: t.strftime(\"%Y%m%d\") , DATES))\n",
    "print \"SIZE:\", len(DATES), \"range:\", DATES\n",
    "print DATES_STR\n",
    "dates_start = arrow.get(DATES[0].strftime(\"%Y-%m-%d\"), \"YYYY-MM-DD\").to(\"Asia/Shanghai\").floor('day').timestamp\n",
    "dates_end = arrow.get(DATES[-1].strftime(\"%Y-%m-%d\"), \"YYYY-MM-DD\").to(\"Asia/Shanghai\").ceil('day').timestamp\n",
    "print \"DATES[0]\", DATES[0], \"DATES[-1]\", DATES[-1], \"dates_start\", dates_start, \"dates_end\", dates_end\n",
    "\n",
    "\n",
    "CITY = 'bj'\n",
    "print \"city:\", CITY\n",
    "\n",
    "order_dump_path = \"{hdfs_dump_base_path}/order_{dt}_{n}days.parquet\".format(dt=base_dt, n=days, hdfs_dump_base_path=hdfs_dump_base_path)\n",
    "print \"order dump:\", order_dump_path\n",
    "\n",
    "dispatch_info_dump_path = \"{hdfs_dump_base_path}/dispatch_info_{dt}_{n}days.parquet\".format(dt=base_dt, n=days, hdfs_dump_base_path=hdfs_dump_base_path)\n",
    "print \"dispatch info dump:\", dispatch_info_dump_path\n",
    "\n",
    "dispatch_dump_path = \"{hdfs_dump_base_path}/dispatch_{dt}_{n}days.parquet\".format(dt=base_dt, n=days, hdfs_dump_base_path=hdfs_dump_base_path)\n",
    "print \"dispatch merged dump:\", dispatch_dump_path\n",
    "\n",
    "bidding_dump_path = \"{hdfs_dump_base_path}/bidding_{dt}_{n}days.parquet\".format(dt=base_dt, n=days, hdfs_dump_base_path=hdfs_dump_base_path)\n",
    "print \"bidding merged dump:\", bidding_dump_path\n",
    "\n",
    "order_allInOne_path = \"{hdfs_dump_base_path}/order_all_in_one_{dt}_{n}days.parquet\".format(dt=base_dt, n=days, hdfs_dump_base_path=hdfs_dump_base_path)\n",
    "print \"order+dispatch dump:\", order_allInOne_path\n",
    "\n",
    "order_allInOne_with_supply_path = \"{hdfs_dump_base_path}/order_all_in_one_supply_{dt}_{n}days.parquet\".format(dt=base_dt, n=days, hdfs_dump_base_path=hdfs_dump_base_path)\n",
    "print \"order_merged + supply dump:\", order_allInOne_with_supply_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构造spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "+--------------------+-----------+\n",
      "|           tableName|isTemporary|\n",
      "+--------------------+-----------+\n",
      "|      bidding_access|      false|\n",
      "|     dispatch_detail|      false|\n",
      "|       dispatch_info|      false|\n",
      "|driver_api_access...|      false|\n",
      "|         order_track|      false|\n",
      "|   personal_dispatch|      false|\n",
      "|        pre_dispatch|      false|\n",
      "|       service_order|      false|\n",
      "|   service_order_ext|      false|\n",
      "|     system_dispatch|      false|\n",
      "+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import asc, desc\n",
    "from pyspark.sql import Row\n",
    "\n",
    "hdfs_python_lib_path = \"hdfs:///libs/pyspark\"\n",
    "\n",
    "yarn_dist_files = [\n",
    "    \"pyspark.zip\",\n",
    "    \"py4j-0.10.3-src.zip\",\n",
    "]\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .master(\"yarn-client\") \\\n",
    "        .appName(\"orderBase_bidding_clean\") \\\n",
    "        .config(\"spark.ui.port\", \"4200\") \\\n",
    "        .config(\"spark.driver.memory\", \"2g\") \\\n",
    "        .config(\"spark.executor.instances\", \"7\") \\\n",
    "        .config(\"spark.executor.cores\", \"4\") \\\n",
    "        .config(\"spark.executor.memory\", \"8g\") \\\n",
    "        .config(\"spark.default.parallelism\", str(PARALLEL)) \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", str(PARALLEL)) \\\n",
    "        .config(\"spark.sql.crossJoin.enabled\", \"true\") \\\n",
    "        .config(\"spark.yarn.dist.files\", ','.join(map(lambda x: hdfs_python_lib_path+\"/\"+x, yarn_dist_files))) \\\n",
    "        .config(\"spark.yarn.appMasterEnv.PYTHONPATH\", ':'.join(yarn_dist_files)) \\\n",
    "        .config(\"spark.yarn.appMasterEnv.PYSPARK_PYTHON\", \"/usr/local/bin/python\") \\\n",
    "        .config(\"spark.executorEnv.PYTHONPATH\", ':'.join(yarn_dist_files)) \\\n",
    "        .config(\"spark.executorEnv.PYSPARK_PYTHON\", \"/usr/local/bin/python\") \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark.sparkContext.addPyFile(\"hdfs:///libs/pyspark/Geohash.zip\")        \n",
    "\n",
    "spark.sql(\"use ods\").show()\n",
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step 1. 订单数据源 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-1. 订单源-service_order（仅终结状态订单）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    where = \" where dt={dt} and city='bj' and status in (7,8) and create_time between {start} and {end}\".format(dt=base_dt, start=base_start, end=base_end)\n",
    "else:\n",
    "    where = \"where dt in ({dates}) and city='{city}' and status in (7,8) and create_time between {start} and {end}\".format(dates=DATES_STR, city=CITY, start=dates_start, end=dates_end)\n",
    "\n",
    "sql = \"\"\"\n",
    "select \n",
    "dt,\n",
    "city,  -- 这是下单所在城市，和dispatch的司机所在城市可能匹配不上\n",
    "service_order_id,\n",
    "product_type_id,\n",
    "is_asap,\n",
    "flag as order_flag,\n",
    "account_id,\n",
    "user_id,\n",
    "driver_id,\n",
    "corporate_id,\n",
    "car_type_id,\n",
    "car_type,\n",
    "car_brand,\n",
    "status as order_status,\n",
    "rc_status,\n",
    "reason_id as order_reason_id,\n",
    "create_time as order_create_time,\n",
    "arrival_time,\n",
    "confirm_time,\n",
    "start_time,\n",
    "end_time,\n",
    "start_position,\n",
    "start_address,\n",
    "end_position,\n",
    "end_address,\n",
    "expect_start_latitude,\n",
    "expect_start_longitude,\n",
    "expect_end_latitude,\n",
    "expect_end_longitude,\n",
    "start_latitude,\n",
    "start_longitude,\n",
    "end_latitude,\n",
    "end_longitude,\n",
    "coupon_name,\n",
    "coupon_facevalue,\n",
    "dependable_distance, --调整后里程\n",
    "actual_time_length --调整后时长\n",
    "from service_order {where}\n",
    "\"\"\".format(where=where)\n",
    "\n",
    "# 执行sparkSQL\n",
    "orderDF = spark.sql(sql)\n",
    "\n",
    "# 创建临时表并cache\n",
    "orderDF.createOrReplaceTempView(\"v_order\")\n",
    "spark.catalog.cacheTable(\"v_order\")\n",
    "\n",
    "print \"original:\", orderDF.count()\n",
    "\n",
    "# 按时间戳去重（partition的分割不同日期可能会有重复的）\n",
    "orderDF = orderDF.orderBy(\"service_order_id\", desc(\"update_time\")).dropDuplicates(['service_order_id']).drop('update_time')\n",
    "print \"dropDuplicates:\", orderDF.count()\n",
    "\n",
    "\n",
    "orderDF.limit(5).toPandas().head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-2. 订单扩展源-service_order_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    where = \" where a.dt={dt} and a.service_order_id in (select service_order_id from v_order)\".format(dt=base_dt)\n",
    "else:\n",
    "    where = \"where a.dt in ({dates}) and a.service_order_id in (select service_order_id from v_order)\".format(dates=DATES_STR)\n",
    "\n",
    "sql = \"\"\"\n",
    "select \n",
    "a.service_order_id,\n",
    "a.update_time,\n",
    "a.user_type,\n",
    "a.predict_amount,\n",
    "a.deadhead_distance,\n",
    "b.distance as estimate_distance,\n",
    "b.estimate_price\n",
    "from service_order_ext a\n",
    "lateral view json_tuple(a.estimate_snap, 'time_length', 'distance', 'estimate_price') b as time_length, distance, estimate_price\n",
    "{where}\n",
    "\"\"\".format(where=where)\n",
    "\n",
    "print sql\n",
    "\n",
    "# 执行sparkSQL\n",
    "orderExtDF = spark.sql(sql)\n",
    "\n",
    "# 创建临时表并cache\n",
    "orderExtDF.createOrReplaceTempView(\"v_order_ext\")\n",
    "spark.catalog.cacheTable(\"v_order_ext\")\n",
    "\n",
    "print \"original:\", orderExtDF.count()\n",
    "\n",
    "# 按时间戳去重（partition的分割不同日期可能会有重复的）\n",
    "orderExtDF = orderExtDF.orderBy(\"service_order_id\", desc(\"update_time\")).dropDuplicates(['service_order_id']).drop('update_time')\n",
    "print \"dropDuplicates:\", orderExtDF.count()\n",
    "\n",
    "orderExtDF.limit(5).toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-3 合并订单数据 （service_order left join service_order_ext）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "select \n",
    "a.*,\n",
    "b.service_order_id as service_order_id_ext,\n",
    "b.user_type,\n",
    "b.predict_amount,\n",
    "b.deadhead_distance,\n",
    "b.estimate_distance,\n",
    "b.estimate_price\n",
    "from v_order a left join v_order_ext b \n",
    "on a.service_order_id = b.service_order_id\n",
    "\"\"\"\n",
    "\n",
    "# 执行sparkSQL\n",
    "orderMergeDF = spark.sql(sql).drop('service_order_id_ext')\n",
    "\n",
    "# 创建临时表并cache\n",
    "orderMergeDF.createOrReplaceTempView(\"v_order_all\")\n",
    "spark.catalog.cacheTable(\"v_order_all\")\n",
    "\n",
    "print \"orginal:\", orderMergeDF.count()\n",
    "\n",
    "orderMergeDF = orderMergeDF.dropDuplicates(['service_order_id'])\n",
    "print \"dropDuplicates:\", orderMergeDF.count()\n",
    "\n",
    "\n",
    "print orderMergeDF.count()\n",
    "orderMergeDF.limit(5).toPandas().head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 导出为parquet文件\n",
    "orderMergeDF.filter(\"estimate_distance is not null\").write.mode('overwrite').parquet(order_dump_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 导出确认\n",
    "spark.sql(\"select * from `parquet`.`{infile}`\".format(infile=order_dump_path)).limit(5).toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 清理spark的缓存\n",
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 2. 派单源"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-1 dispatch_info "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    where = \" where dt={dt} and c.city='bj'\".format(dt=base_dt)\n",
    "else:\n",
    "    where = \"where dt in ({dates}) and c.city='{city}' \".format(dates=DATES_STR, city=CITY)\n",
    "\n",
    "# 这个在detail才对，这里是错的，因为info的日志是按round记录（每round记录最后1个batch的累计量）\n",
    "# row_number() over (distribute by service_order_id sort by round, batch) as acc_batch, \n",
    "sql = \"\"\"\n",
    "select \n",
    "c.city,\n",
    "a.service_order_id,\n",
    "a.round,\n",
    "a.batch,\n",
    "ROW_NUMBER() over (distribute by service_order_id sort by round, batch) as acc_batch,\n",
    "unix_timestamp(a.datetime,'yyyy-MM-dd HH:mm:ss') as datetime,\n",
    "a.dispatch_time,\n",
    "a.decision_time,\n",
    "a.create_time,\n",
    "a.update_time,\n",
    "a.status,\n",
    "a.bidding_id,\n",
    "a.dispatch_type,\n",
    "a.decision_type,\n",
    "a.dispatch_count,\n",
    "a.response_count,\n",
    "a.accept_count,\n",
    "a.flag,\n",
    "a.user_level,\n",
    "a.user_gender,\n",
    "a.can_dispatch_count,\n",
    "a.decision_driver_id,\n",
    "a.decision_car_type_id,\n",
    "b.driver_bidding_rate,\n",
    "b.user_bidding_rate,\n",
    "b.driver_estimate_price,\n",
    "c.code,\n",
    "c.isRushHour,\n",
    "c.remark,\n",
    "d.assign_max_range, \n",
    "d.batch_driver_count, \n",
    "d.batch_interval, \n",
    "d.contribution_pct, \n",
    "d.distanct_pct,\n",
    "d.evaluation_pct,\n",
    "d.max_accept_count,\n",
    "d.max_driver_count,\n",
    "d.max_range\n",
    "from dispatch_info a\n",
    "lateral view json_tuple(a.add_price_info, 'dsb', 'sb', 'dep') b as driver_bidding_rate, user_bidding_rate, driver_estimate_price\n",
    "lateral view json_tuple(a.template_snapshot, 'city', 'code', 'dispatchParams', 'isRushHour', 'remark') c as city, code, dispatchParams, isRushHour,remark\n",
    "lateral view json_tuple(c.dispatchParams, 'aSSIGN_MAX_RANGE', 'bATCH_DRIVER_COUNT', 'bATCH_INTERVAL', 'cONTRIBUTION','dISTANCE','eVALUATION','mAX_ACCEPT_COUNT','mAX_DRIVER_COUNT','mAX_RANGE') d as assign_max_range, batch_driver_count, batch_interval, contribution_pct, distanct_pct,evaluation_pct,max_accept_count,max_driver_count,max_range\n",
    "{where}\n",
    "\"\"\".format(where=where)\n",
    "\n",
    "# 执行sparkSQL\n",
    "spark.sql(sql).createOrReplaceTempView(\"v_dispatch_info\")\n",
    "spark.catalog.cacheTable(\"v_dispatch_info\")\n",
    "\n",
    "print \"original:\", spark.sql(\"select * from v_dispatch_info\").count()\n",
    "spark.sql(\"select * from v_dispatch_info\").limit(5).toPandas().head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dispatch_info cleansing  （按订单获取统计量：首轮响应、首轮撮合成功、末轮）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 派单基准\n",
    "sql = \"\"\"\n",
    "select\n",
    "city,\n",
    "service_order_id,\n",
    "bidding_id,\n",
    "acc_batch as sum_batch,\n",
    "create_time as dispatch_create_time,\n",
    "status as dispatch_status,\n",
    "flag as dispatch_flag,\n",
    "-- add_price_redispatch,\n",
    "dispatch_type,\n",
    "decision_type,\n",
    "user_level,\n",
    "user_gender,\n",
    "isRushHour,\n",
    "remark,\n",
    "batch_driver_count, \n",
    "batch_interval, \n",
    "contribution_pct, \n",
    "distanct_pct,\n",
    "evaluation_pct,\n",
    "max_range\n",
    "from v_dispatch_info\n",
    "\"\"\"\n",
    "baseDF = spark.sql(sql).orderBy(\"service_order_id\", desc(\"decision_time\")).dropDuplicates(['service_order_id']).drop('decision_time')\n",
    "print \"base:\", baseDF.count()\n",
    "# print baseDF.limit(3).show()\n",
    "\n",
    "# 订单累计值\n",
    "sql = \"\"\"\n",
    "select \n",
    "service_order_id,\n",
    "max(round) as max_round,\n",
    "round(avg(can_dispatch_count), 0) as avg_can_dispatch,   --平均每轮可派司机数\n",
    "sum(can_dispatch_count) as sum_can_dispatch,             --累计可派司机数\n",
    "round(avg(dispatch_count), 0) as avg_dispatch_count,\n",
    "sum(dispatch_count) as sum_dispatch,\n",
    "round(avg(response_count), 0) as avg_response_count,\n",
    "sum(response_count) as sum_response,\n",
    "sum(accept_count) as sum_accept\n",
    "from v_dispatch_info\n",
    "group by service_order_id\n",
    "\"\"\"\n",
    "totalDF = spark.sql(sql)\n",
    "# print totalDF.limit(3).show()\n",
    "\n",
    "outDF = baseDF.join(totalDF, baseDF.service_order_id == totalDF.service_order_id, 'left').drop(totalDF.service_order_id)\n",
    "# print \"merge:\",outDF.count()\n",
    "# outDF.limit(10).toPandas().head()\n",
    "\n",
    "\n",
    "# 首轮司机响应（但用户不一定决策成功）\n",
    "sql = \"\"\"\n",
    "select\n",
    "service_order_id,\n",
    "round as first_accept_round,\n",
    "acc_batch as first_accept_acc_batch,\n",
    "decision_time as first_accept_time,\n",
    "driver_bidding_rate as first_accept_driver_bidding_rate,\n",
    "user_bidding_rate as first_accept_user_bidding_rate,\n",
    "driver_estimate_price as first_accept_driver_estimate_price\n",
    "from v_dispatch_info\n",
    "where accept_count>0 \n",
    "\"\"\"\n",
    "# @Deprecated 未决策成功的加价倍率没有记录，改为后面从dispatch_detail取\n",
    "firstAcceptDF = spark.sql(sql).orderBy(\"service_order_id\", asc(\"first_accept_time\")).dropDuplicates(['service_order_id'])\n",
    "outDF = outDF.join(firstAcceptDF, outDF.service_order_id == firstAcceptDF.service_order_id, 'left').drop(firstAcceptDF.service_order_id)\n",
    "# print \"merge:\",outDF.count()\n",
    "# print outDF.filter(\"first_accept_time is not null\").limit(10).toPandas().head()\n",
    "\n",
    "\n",
    "# 首轮决策成功\n",
    "sql = \"\"\"\n",
    "select\n",
    "service_order_id,\n",
    "round as first_decision_round,\n",
    "acc_batch as first_decision_acc_batch,\n",
    "decision_time as first_decision_time,\n",
    "decision_driver_id as first_decision_driver_id,\n",
    "decision_car_type_id as first_decision_car_type_id, \n",
    "driver_bidding_rate as first_decision_driver_bidding_rate,\n",
    "user_bidding_rate as first_decision_user_bidding_rate,\n",
    "driver_estimate_price as first_decision_driver_estimate_price\n",
    "from v_dispatch_info\n",
    "where decision_driver_id>0\n",
    "\"\"\"\n",
    "\n",
    "# @Deprecated 未决策成功的加价倍率没有记录，改为后面从dispatch_detail取\n",
    "\n",
    "firstDecisionDF = spark.sql(sql).orderBy(\"service_order_id\", asc(\"first_decision_time\")).dropDuplicates(['service_order_id'])\n",
    "outDF = outDF.join(firstDecisionDF, outDF.service_order_id == firstDecisionDF.service_order_id, 'left').drop(firstDecisionDF.service_order_id)\n",
    "# print \"merge:\",outDF.count()\n",
    "# print outDF.filter(\"first_decision_driver_id is not null\").limit(10).toPandas().head()\n",
    "\n",
    "\n",
    "# # 最后1轮\n",
    "sql = \"\"\"\n",
    "select \n",
    "service_order_id,\n",
    "round as final_round,\n",
    "case when decision_time>0 then decision_time else datetime end as final_decision_time,\n",
    "decision_driver_id as final_decision_driver_id,\n",
    "decision_car_type_id as final_decision_car_type_id,\n",
    "status as final_dispatch_status,\n",
    "driver_bidding_rate as final_driver_bidding_rate,\n",
    "user_bidding_rate as final_user_bidding_rate,\n",
    "driver_estimate_price as final_driver_estimate_price\n",
    "from v_dispatch_info\n",
    "\"\"\"\n",
    "\n",
    "# @Deprecated 未决策成功的加价倍率没有记录，改为后面从dispatch_detail取\n",
    "\n",
    "finalDF = spark.sql(sql).orderBy(\"service_order_id\", desc(\"final_decision_time\")).dropDuplicates(['service_order_id'])\n",
    "outDF = outDF.join(finalDF, outDF.service_order_id == finalDF.service_order_id, 'left').drop(finalDF.service_order_id)\n",
    "\n",
    "print \"merge:\",outDF.count()\n",
    "outDF.filter(\"final_decision_driver_id>0\").limit(10).toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 导出为parquet文件\n",
    "outDF.repartition(PARALLEL).write.mode('overwrite').parquet(dispatch_info_dump_path)\n",
    "\n",
    "print \"dump:\", dispatch_info_dump_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 确认导出结果\n",
    "spark.sql(\"select * from `parquet`.`{path}`\".format(path=dispatch_info_dump_path)).limit(10).toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 清理spark的缓存\n",
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-2 dispatch detail "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    where = \" where dt={dt} and city='bj'\".format(dt=base_dt)\n",
    "else:\n",
    "    where = \"where dt in ({dates}) and city='{city}' \".format(dates=DATES_STR, city=CITY)\n",
    "\n",
    "sql = \"\"\"\n",
    "select\n",
    "unix_timestamp(a.datetime,'yyyy-MM-dd HH:mm:ss') as datetime,\n",
    "a.service_order_id,\n",
    "a.round,\n",
    "a.batch,\n",
    "a.driver_id,\n",
    "a.dispatch_time,\n",
    "a.response_time,\n",
    "a.accept_status,\n",
    "a.driver_bidding_rate,\n",
    "a.driver_estimate_price,\n",
    "a.decision_time,\n",
    "a.decision_result,\n",
    "a.is_assigned,\n",
    "a.route_distance,\n",
    "a.route_time_length,\n",
    "a.distance,\n",
    "a.distance_time_length\n",
    "-- b.car_type_id,\n",
    "-- b.brand,\n",
    "-- b.driver_level\n",
    "from dispatch_detail a\n",
    "-- lateral view json_tuple(a.dispatch_snapshot, 'car_type_id', 'brand', 'driver_level', 'distance_rate', 'contribution_rate', 'evaluation_rate', 'base_score', 'evaluation', 'contribution') b as car_type_id, brand, driver_level, distance_rate, contribution_rate, evaluation_rate, base_score, evaluation, contribution\n",
    "{where}\n",
    "\"\"\".format(where=where)\n",
    "\n",
    "# print sql\n",
    "\n",
    "# 执行sparkSQL\n",
    "spark.sql(sql).createOrReplaceTempView(\"v_dispatch_detail\")\n",
    "spark.catalog.cacheTable(\"v_dispatch_detail\")\n",
    "\n",
    "# 结果核对\n",
    "print \"original:\", spark.sql(\"select * from v_dispatch_detail\").count()\n",
    "spark.sql(\"select * from v_dispatch_detail\").limit(5).toPandas().head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dispatch_detail cleansing （按订单获取统计量：首轮响应、首轮撮合成功、末轮）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 按batch聚合每一batch的派单情况，形成和dispath_info类似的格式\n",
    "\n",
    "sql = \"\"\"\n",
    "select \n",
    "service_order_id, \n",
    "round, \n",
    "batch,\n",
    "max(datetime) as datetime,\n",
    "ROW_NUMBER() over (distribute by service_order_id sort by round, batch) as acc_batch,\n",
    "count(driver_id) as dispatch_count,\n",
    "max(dispatch_time) as dispatch_time,\n",
    "-- 本来响应司机数，时间用最大时间近似\n",
    "sum(case when accept_status=1 then 1 else 0 end) as accept_count,  \n",
    "max(response_time) as response_time,\n",
    "-- 本轮有决策成功的，时间用最大时间近似\n",
    "max(case when decision_result=2 then 1 else 0 end) as decision_complete, \n",
    "max(decision_time) as decision_time, \n",
    "-- 加价相关\n",
    "min(driver_bidding_rate) as min_bidding_rate,\n",
    "--mean(driver_bidding_rate) as avg_bidding_rate,\n",
    "max(driver_bidding_rate) as max_bidding_rate,\n",
    "min(driver_estimate_price) as min_price,\n",
    "--mean(driver_estimate_price) as avg_price,\n",
    "max(driver_estimate_price) as max_price,\n",
    "-- 距离及时间相关\n",
    "min(route_distance) as min_route_distance,\n",
    "mean(route_distance) as avg_route_distance,\n",
    "max(route_distance) as max_route_distance,\n",
    "min(route_time_length) as min_route_time_length,\n",
    "mean(route_time_length) as avg_route_time_length,\n",
    "max(route_time_length) as max_route_time_length,\n",
    "min(distance) as min_distance,\n",
    "mean(distance) as avg_distance,\n",
    "max(distance) as max_distance,\n",
    "min(distance_time_length) as min_distance_time_length,\n",
    "mean(distance_time_length) as avg_distance_time_length,\n",
    "max(distance_time_length) as max_distance_time_length\n",
    "from v_dispatch_detail \n",
    "group by service_order_id, round, batch\n",
    "\"\"\".format()\n",
    "\n",
    "# 执行sparkSQL\n",
    "spark.sql(sql).createOrReplaceTempView(\"v_dispatch_detail_grouped\")\n",
    "spark.catalog.cacheTable(\"v_dispatch_detail_grouped\")\n",
    "\n",
    "# 释放原始表的缓存，只保留groupby之后的即可\n",
    "spark.catalog.uncacheTable(\"v_dispatch_detail\")\n",
    "\n",
    "print \"service_order_id in dispatch_detail:\", spark.sql(\"select * from v_dispatch_detail_grouped\").count()\n",
    "\n",
    "spark.sql(\"select * from v_dispatch_detail_grouped\").limit(5).toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 以 2-1 计算的dispatch_info为基准，做连接dispatch_detail统计的加价相关数据\n",
    "\n",
    "##### dispatch_info 在 步骤 2-1计算完成后写入了parquet文件，现在从文件中读取出来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 读取dispatch_info，按订单聚合的结果\n",
    "\n",
    "dispatchInfoDF = spark.sql(\"select * from `parquet`.`{path}`\".format(path=dispatch_info_dump_path))\n",
    "print \"dispatchInfoDF:\", dispatchInfoDF.count()\n",
    "\n",
    "# print dispatchInfoDF.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 首轮司机响应（但用户不一定决策成功）\n",
    "sql = \"\"\"\n",
    "select\n",
    "service_order_id,\n",
    "round as first_accept_round_ext,\n",
    "acc_batch as first_accept_acc_batch_ext,\n",
    "response_time as first_accept_time_ext,  \n",
    "-- 加价相关\n",
    "min_bidding_rate as first_accept_min_bidding_rate,\n",
    "max_bidding_rate as first_accept_max_bidding_rate,\n",
    "min_price as first_accept_min_price,\n",
    "max_price as first_accept_max_price,\n",
    "-- 距离及时间相关\n",
    "min_route_distance as first_accept_min_route_distance,\n",
    "max_route_distance as first_accept_max_route_distance,\n",
    "min_route_time_length as first_accept_min_route_time_length,\n",
    "avg_route_time_length as first_accept_avg_route_time_length,\n",
    "max_route_time_length as first_accept_max_route_time_length,\n",
    "min_distance as first_accept_min_distance,\n",
    "avg_distance as first_accept_avg_distance,\n",
    "max_distance as first_accept_max_distance,\n",
    "min_distance_time_length as first_accept_min_distance_time_length,\n",
    "avg_distance_time_length as first_accept_avg_distance_time_length,\n",
    "max_distance_time_length as first_accept_max_distance_time_length\n",
    "from v_dispatch_detail_grouped\n",
    "where accept_count>0 \n",
    "\"\"\"\n",
    "\n",
    "firstAcceptDF = spark.sql(sql).orderBy(\"service_order_id\", asc(\"first_accept_time_ext\")).dropDuplicates(['service_order_id'])\n",
    "print \"firstAcceptDF uniq:\", firstAcceptDF.count()\n",
    "outDF = dispatchInfoDF.join(firstAcceptDF, dispatchInfoDF.service_order_id == firstAcceptDF.service_order_id, 'left').drop(firstAcceptDF.service_order_id)\n",
    "print \"merge:\",outDF.count()\n",
    "print \"first_accept_time_ext is not null\", outDF.filter(\"first_accept_time_ext is not null\").count()\n",
    "\n",
    "\n",
    "# 首轮决策成功\n",
    "sql = \"\"\"\n",
    "select\n",
    "service_order_id,\n",
    "round as first_decision_round_ext,\n",
    "acc_batch as first_decision_acc_batch_ext,\n",
    "decision_time as first_decision_time_ext,  \n",
    "-- 加价相关\n",
    "min_bidding_rate as first_decision_min_bidding_rate,\n",
    "max_bidding_rate as first_decision_max_bidding_rate,\n",
    "min_price as first_decision_min_price,\n",
    "max_price as first_decision_max_price,\n",
    "-- 距离及时间相关\n",
    "min_route_distance as first_decision_min_route_distance,\n",
    "avg_route_distance as first_decision_avg_route_distance,\n",
    "max_route_distance as first_decision_max_route_distance,\n",
    "min_route_time_length as first_decision_min_route_time_length,\n",
    "avg_route_time_length as first_decision_avg_route_time_length,\n",
    "max_route_time_length as first_decision_max_route_time_length,\n",
    "min_distance as first_decision_min_distance,\n",
    "avg_distance as first_decision_avg_distance,\n",
    "max_distance as first_decision_max_distance,\n",
    "min_distance_time_length as first_decision_min_distance_time_length,\n",
    "avg_distance_time_length as first_decision_avg_distance_time_length,\n",
    "max_distance_time_length as first_decision_max_distance_time_length\n",
    "from v_dispatch_detail_grouped\n",
    "where decision_complete>0\n",
    "\"\"\"\n",
    "\n",
    "# @Deprecated 未决策成功的加价倍率没有记录，改为后面从dispatch_detail取\n",
    "\n",
    "firstDecisionDF = spark.sql(sql) \\\n",
    "                    .orderBy(\"service_order_id\", asc(\"first_decision_time_ext\")) \\\n",
    "                    .dropDuplicates(['service_order_id'])\n",
    "print \"firstDecisionDF uniq:\", firstDecisionDF.count()\n",
    "outDF = outDF.join(firstDecisionDF, outDF.service_order_id == firstDecisionDF.service_order_id, 'left') \\\n",
    "                .drop(firstDecisionDF.service_order_id)\n",
    "print \"merge:\",outDF.count()\n",
    "print \"first_decision_time_ext is not null\", outDF.filter(\"first_decision_time_ext is not null\").count()\n",
    "\n",
    "\n",
    "# # 最后1轮\n",
    "sql = \"\"\"\n",
    "select \n",
    "service_order_id,\n",
    "round as final_round_ext,\n",
    "acc_batch as final_acc_batch_ext,\n",
    "datetime as final_time_ext,  \n",
    "-- 加价相关\n",
    "min_bidding_rate as final_min_bidding_rate,\n",
    "max_bidding_rate as final_max_bidding_rate,\n",
    "min_price as final_min_price,\n",
    "max_price as final_max_price,\n",
    "-- 距离及时间相关\n",
    "min_route_distance as final_min_route_distance,\n",
    "avg_route_distance as final_avg_route_distance,\n",
    "max_route_distance as final_max_route_distance,\n",
    "min_route_time_length as final_min_route_time_length,\n",
    "avg_route_time_length as final_avg_route_time_length,\n",
    "max_route_time_length as final_max_route_time_length,\n",
    "min_distance as final_min_distance,\n",
    "avg_distance as final_avg_distance,\n",
    "max_distance as final_max_distance,\n",
    "min_distance_time_length as final_min_distance_time_length,\n",
    "avg_distance_time_length as final_avg_distance_time_length,\n",
    "max_distance_time_length as final_max_distance_time_length\n",
    "from v_dispatch_detail_grouped\n",
    "\"\"\"\n",
    "\n",
    "# @Deprecated 未决策成功的加价倍率没有记录，改为后面从dispatch_detail取\n",
    "\n",
    "finalDF = spark.sql(sql).orderBy(\"service_order_id\", desc(\"final_time_ext\")).dropDuplicates(['service_order_id'])\n",
    "print \"finalDF uniq:\", finalDF.count()\n",
    "outDF = outDF.join(finalDF, outDF.service_order_id == finalDF.service_order_id, 'left').drop(finalDF.service_order_id)\n",
    "print \"merge:\",outDF.count()\n",
    "print \"final_time_ext is not null\", outDF.filter(\"final_time_ext is not null\").count()\n",
    "\n",
    "outDF.filter(\"final_decision_driver_id>0\").limit(10).toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print dispatch_dump_path\n",
    "\n",
    "# 导出为parquet文件\n",
    "outDF.repartition(PARALLEL).write.mode('overwrite').parquet(dispatch_dump_path)\n",
    "\n",
    "spark.sql(\"select * from `parquet`.`{path}`\".format(path=dispatch_dump_path)).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 清理spark的缓存\n",
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##  step 3. 加价源"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-1 加价前-pre_dispatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 从dispatch_info提取加价id，存为临时表\n",
    "if DEBUG:\n",
    "    dep_where = \"where dt={dt} and city='bj'\".format(dt=base_dt)\n",
    "else:\n",
    "    dep_where = \"where dt in ({dates}) and city='{city}' \".format(dates=DATES_STR, city=CITY)\n",
    "\n",
    "\n",
    "# 根据bidding_id提取pre_dispatch记录\n",
    "if DEBUG:\n",
    "    where = \"where dt={dt}\".format(dt=base_dt)\n",
    "else:\n",
    "    where = \"where dt in ({dates}) \".format(dates=DATES_STR)\n",
    "# 按bidding_id过滤\n",
    "where += \" and bidding_id in (select distinct bidding_id from ods.dispatch_info {dep_where})\".format(dep_where=dep_where)\n",
    "\n",
    "sql = \"\"\"\n",
    "select\n",
    "unix_timestamp(a.datetime,'yyyy-MM-dd_HH:mm:ss') as datetime,\n",
    "a.bidding_id,\n",
    "b.productTypeId as pre_productTypeId,\n",
    "b.estimatePrice as pre_estimatePrice,\n",
    "b.availableDriverNum as pre_availableDriverNum,\n",
    "b.M as pre_M\n",
    "from (\n",
    "    select\n",
    "    datetime,\n",
    "    bidding_id,\n",
    "    regexp_replace(data, '\\\\\\\\\\\\\\\\\\\"', '\"') as data\n",
    "    from pre_dispatch\n",
    "    {where}\n",
    ") a\n",
    "lateral view json_tuple(a.data, 'productTypeId', 'estimatePrice', 'availableDriverNum', 'M', 'M1', 'M2', 'M3', 'a', 'b', 'c', 'T', 'N', 'L') b as productTypeId, estimatePrice, availableDriverNum, M, M1, M2, M3, a, b, c, T, N, L\n",
    "\"\"\".format(where=where)\n",
    "\n",
    "bidding_base_df = spark.sql(sql).dropDuplicates(['bidding_id'])\n",
    "# bidding_base_df.cache()\n",
    "print  bidding_base_df.count()\n",
    "\n",
    "spark.sql(sql).limit(5).toPandas().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-2 系统决策加价-system_dispatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 从dispatch_info提取加价id，存为临时表\n",
    "if DEBUG:\n",
    "    dep_where = \"where dt={dt} and city='bj' and decision_type=1\".format(dt=base_dt)\n",
    "else:\n",
    "    dep_where = \"where dt in ({dates}) and city='{city}'and decision_type=1\".format(dates=DATES_STR, city=CITY)\n",
    "\n",
    "\n",
    "# 根据bidding_id提取pre_dispatch记录\n",
    "if DEBUG:\n",
    "    where = \"where dt={dt}\".format(dt=base_dt)\n",
    "else:\n",
    "    where = \"where dt in ({dates}) \".format(dates=DATES_STR)\n",
    "# 按bidding_id过滤\n",
    "where += \" and bidding_id in (select distinct bidding_id from ods.dispatch_info {dep_where})\".format(dep_where=dep_where)\n",
    "\n",
    "\n",
    "sql = \"\"\"\n",
    "select\n",
    "a.bidding_id,\n",
    "a.round as end_bidding_round,\n",
    "b.totalMagnification as end_min_bidding_rate,\n",
    "b.totalMagnification as end_max_bidding_rate\n",
    "--(b.totalMagnification * (1 - b.commission_rate)) as end_min_bidding_rate,\n",
    "--(b.totalMagnification * (1 - b.commission_rate)) as end_max_bidding_rate\n",
    "from (\n",
    "    select\n",
    "    datetime,\n",
    "    order_id,\n",
    "    bidding_id,\n",
    "    round,\n",
    "    regexp_replace(data, '\\\\\\\\\\\\\\\\\\\"', '\"') as json\n",
    "    from system_dispatch\n",
    "    {where} and order_id is not null\n",
    ") a\n",
    "lateral view json_tuple(a.json, 'startBiddingRound', 'commission_rate', 'totalMagnification', 'M', 'N', 'L', 'O', 'D') b as startBiddingRound, commission_rate, totalMagnification, M, N, L, O, D\n",
    "\"\"\".format(where=where)\n",
    "\n",
    "# print sql\n",
    "# print sql\n",
    "\n",
    "sys_bidding_df = spark.sql(sql).orderBy(\"bidding_id\", desc('end_bidding_round')).dropDuplicates(['bidding_id'])\n",
    "# sys_bidding_df.cache()\n",
    "print sys_bidding_df.count()\n",
    "\n",
    "spark.sql(sql).filter(\"bidding_id=6409357301886353828\").orderBy(\"bidding_id\", desc('end_bidding_round')).dropDuplicates(['bidding_id']).limit(5).toPandas().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-3 人工计策加价-personal_dispatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 从dispatch_info提取加价id，存为临时表\n",
    "if DEBUG:\n",
    "    dep_where = \"where dt={dt} and city='bj' and decision_type=2\".format(dt=base_dt)\n",
    "else:\n",
    "    dep_where = \"where dt in ({dates}) and city='{city}'and decision_type=2\".format(dates=DATES_STR, city=CITY)\n",
    "\n",
    "\n",
    "# 根据bidding_id提取pre_dispatch记录\n",
    "if DEBUG:\n",
    "    where = \"where dt={dt}\".format(dt=base_dt)\n",
    "else:\n",
    "    where = \"where dt in ({dates}) \".format(dates=DATES_STR)\n",
    "# 按bidding_id过滤\n",
    "where += \" and bidding_id in (select distinct bidding_id from ods.dispatch_info {dep_where})\".format(dep_where=dep_where)\n",
    "\n",
    "import json\n",
    "def extract_driver_bidding_rate(row):\n",
    "    commission_rate = row.commission_rate\n",
    "    try:\n",
    "        dirvers = json.loads(row.driverData)\n",
    "        bidding_rates = [val['totalMagnification'] for val in dirvers.values()]\n",
    "    except:\n",
    "        bidding_rates = [0.0]\n",
    "    return Row(bidding_id=row.bidding_id,\n",
    "              end_bidding_round=row.end_bidding_round,\n",
    "              end_min_bidding_rate=min(bidding_rates),\n",
    "              end_max_bidding_rate=max(bidding_rates))\n",
    "\n",
    "sql = \"\"\"\n",
    "select\n",
    "a.bidding_id,\n",
    "a.round as end_bidding_round,\n",
    "b.commission_rate,\n",
    "b.driverData\n",
    "from (\n",
    "    select\n",
    "    datetime,\n",
    "    order_id,\n",
    "    bidding_id,\n",
    "    round,\n",
    "    regexp_replace(data, '\\\\\\\\\\\\\\\\\\\"', '\"') as json\n",
    "    from personal_dispatch\n",
    "    {where} and order_id is not null\n",
    ") a\n",
    "lateral view json_tuple(a.json, 'commission_rate', 'estimateDist', 'M', 'O', 'D', 'driverData') b as commission_rate, estimateDist, M, O, D, driverData\n",
    "\"\"\".format(where=where)\n",
    "\n",
    "# print sql\n",
    "# print sql\n",
    "\n",
    "personal_bidding_df = spark.sql(sql) \\\n",
    "                        .orderBy(\"bidding_id\", desc('end_bidding_round')) \\\n",
    "                        .dropDuplicates(['bidding_id']) \\\n",
    "                        .rdd.map(extract_driver_bidding_rate).toDF()\n",
    "\n",
    "# personal_bidding_df.cache()\n",
    "print personal_bidding_df.count()\n",
    "\n",
    "personal_bidding_df.toPandas().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加价倍率join （以pre_dispatch中的订单为基准，left join 系统决策加价 和 人工决策加价）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"sys_bidding_df:\", sys_bidding_df.count()\n",
    "print \"personal_bidding_df:\", personal_bidding_df.count()\n",
    "\n",
    "bidding_df = sys_bidding_df.union(personal_bidding_df)\n",
    "print \"bidding_df:\", bidding_df.count()\n",
    "\n",
    "print \"bidding_base_df:\", bidding_base_df.count()\n",
    "outDF = bidding_base_df.join(bidding_df, bidding_base_df.bidding_id == bidding_df.bidding_id, 'left') \\\n",
    "                    .drop(bidding_df.bidding_id)\n",
    "# outDF.cache()\n",
    "# bidding_base_df.unpersist()\n",
    "# sys_bidding_df.unpersist()\n",
    "# personal_bidding_df.unpersist()\n",
    "# bidding_df.unpersist()\n",
    "\n",
    "print \"add sys_biding and personal_biding:\", outDF.count()\n",
    "print \"valid bidding count:\", outDF.filter(\"end_max_bidding_rate > 0.0\").count()\n",
    "print \"zero bidding count:\", outDF.filter(\"end_max_bidding_rate=0 or end_max_bidding_rate is null\").count()\n",
    "\n",
    "outDF.limit(3).toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print bidding_dump_path\n",
    "\n",
    "# 导出为parquet文件\n",
    "outDF.repartition(PARALLEL).write.mode('overwrite').parquet(bidding_dump_path)\n",
    "\n",
    "print spark.sql(\"select * from `parquet`.`{path}`\".format(path=bidding_dump_path)).count()\n",
    "\n",
    "# outDF.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. 基础数据merge （订单 left join 派单、加价）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-1 从文件中加载预处理过的订单、派单、加价数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dispatchInfoDF: 1694829\n",
      "orderDF: 1589863\n",
      "biddingDF: 1753761\n"
     ]
    }
   ],
   "source": [
    "dispatchDF = spark.sql(\"select * from `parquet`.`{path}`\".format(path=dispatch_dump_path))\n",
    "print \"dispatchInfoDF:\", dispatchDF.count()\n",
    "\n",
    "orderDF = spark.sql(\"select * from `parquet`.`{path}`\".format(path=order_dump_path))\n",
    "print \"orderDF:\", orderDF.count()\n",
    "\n",
    "biddingDF = spark.sql(\"select * from `parquet`.`{path}`\".format(path=bidding_dump_path))\n",
    "print \"biddingDF:\", biddingDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-2 以订单为基 left joint 派单、加价\n",
    "\n",
    "此步骤容易OOM，若失败请重试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 先释放资源\n",
    "spark.catalog.clearCache()\n",
    "\n",
    "allInOneDF = orderDF.join(dispatchDF, orderDF.service_order_id == dispatchDF.service_order_id, 'left') \\\n",
    "                    .drop(dispatchDF.service_order_id) \\\n",
    "                    .drop(dispatchDF.city)\n",
    "# print \"order + dispatch:\", allInOneDF.count()\n",
    "\n",
    "allInOneDF = allInOneDF.join(biddingDF, allInOneDF.bidding_id == biddingDF.bidding_id, 'left') \\\n",
    "                    .drop(biddingDF.bidding_id)\n",
    "# print \"order + bidding\", allInOneDF.count()\n",
    "\n",
    "\n",
    "allInOneDF.createOrReplaceTempView(\"v_allInOneDF\")\n",
    "# allInOneDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-3 过滤逻辑上存在矛盾的订单"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import asc, desc, expr\n",
    "\n",
    "# 过滤加价日志和派单明细对不上（加价日志缺失）\n",
    "df = spark.sql(\"select * from v_allInOneDF where not (final_max_bidding_rate>0 and end_bidding_round is null)\")\n",
    "\n",
    "# 决策类型改为从flag提取\n",
    "df = df.withColumn(\"decision_type_flag\", expr(\"\"\"\n",
    "case \n",
    "    when (dispatch_flag & 1024) = 1024 then 'system_decision'\n",
    "    when (dispatch_flag & 512) = 512 then 'manual_decision'\n",
    "    else 'unkown'\n",
    "end\n",
    "\"\"\"))\n",
    "\n",
    "# .createOrReplaceTempView(\"v_training\")\n",
    "\n",
    "# spark.catalog.cacheTable(\"v_training\")\n",
    "\n",
    "# spark.catalog.uncacheTable(\"v_miss_bidding\")\n",
    "# print \"orders after filter:\", spark.sql(\"select * from v_training\").count()\n",
    "\n",
    "# spark.sql(\"select dt, count(1) from v_training group by dt order by dt\").show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-4 导出中间结果至parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/user/lujin/order_all_in_one_20170423_14days.parquet\n",
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "| 1289903|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print order_allInOne_path\n",
    "\n",
    "# 导出为parquet文件\n",
    "df.repartition(PARALLEL).write.mode('overwrite').parquet(order_allInOne_path)\n",
    "\n",
    "spark.sql(\"select count(1) from `parquet`.`{path}`\".format(path=order_allInOne_path)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 5. 离线计算高阶特征\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-1 供需数据\n",
    "\n",
    "需求：group by (期望上车点映射5位geohash + 下单时间按5分钟分箱) -> count(service_order_id)\n",
    "\n",
    "相对供给：group by (期望上车点映射5位geohash + 下单时间按5分钟分箱) -> sum(avg_can_dispatch) / count(service_order_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1289903\n",
      "+-------------------+-----------------+---------------------+----------------------+----------------------+\n",
      "|service_order_id   |order_create_time|expect_start_latitude|expect_start_longitude|pre_availableDriverNum|\n",
      "+-------------------+-----------------+---------------------+----------------------+----------------------+\n",
      "|6407619001781635345|1491890056       |39.873821            |116.367985            |151                   |\n",
      "|6408477982352736592|1492090053       |39.964474753926      |116.45419409935       |104                   |\n",
      "|6409239905257511846|1492267452       |40.099064            |116.542989            |33                    |\n",
      "|6410298323350028924|1492513884       |39.998682            |116.489786            |null                  |\n",
      "|6411736892188327215|1492848827       |39.893406            |116.469003            |null                  |\n",
      "+-------------------+-----------------+---------------------+----------------------+----------------------+\n",
      "\n",
      "887710\n"
     ]
    }
   ],
   "source": [
    "# 从parquet加载计算源\n",
    "sql =\"\"\"\n",
    "select \n",
    "service_order_id,\n",
    "order_create_time,\n",
    "expect_start_latitude,\n",
    "expect_start_longitude,\n",
    "pre_availableDriverNum\n",
    "from `parquet`.`{path}`\n",
    "\"\"\".format(path=order_allInOne_path)\n",
    "\n",
    "supply_demand_df = spark.sql(sql)\n",
    "\n",
    "print supply_demand_df.count()\n",
    "\n",
    "supply_demand_df.limit(5).show(truncate=False)\n",
    "\n",
    "# 部分预约单没有可派司机数？\n",
    "print supply_demand_df.filter(\"pre_availableDriverNum > 0\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-1-1 新增key （时间+空间）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>service_order_id</th>\n",
       "      <th>order_create_time</th>\n",
       "      <th>expect_start_latitude</th>\n",
       "      <th>expect_start_longitude</th>\n",
       "      <th>pre_availableDriverNum</th>\n",
       "      <th>geo_time_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6407619001781635345</td>\n",
       "      <td>1491890056</td>\n",
       "      <td>39.873821</td>\n",
       "      <td>116.367985</td>\n",
       "      <td>151</td>\n",
       "      <td>wx4fb_1491861000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6408477982352736592</td>\n",
       "      <td>1492090053</td>\n",
       "      <td>39.964475</td>\n",
       "      <td>116.454194</td>\n",
       "      <td>104</td>\n",
       "      <td>wx4g3_1492061100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6409239905257511846</td>\n",
       "      <td>1492267452</td>\n",
       "      <td>40.099064</td>\n",
       "      <td>116.542989</td>\n",
       "      <td>33</td>\n",
       "      <td>wx4uh_1492238400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      service_order_id  order_create_time  expect_start_latitude  \\\n",
       "0  6407619001781635345         1491890056              39.873821   \n",
       "1  6408477982352736592         1492090053              39.964475   \n",
       "2  6409239905257511846         1492267452              40.099064   \n",
       "\n",
       "   expect_start_longitude pre_availableDriverNum      geo_time_key  \n",
       "0              116.367985                    151  wx4fb_1491861000  \n",
       "1              116.454194                    104  wx4g3_1492061100  \n",
       "2              116.542989                     33  wx4uh_1492238400  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Geohash\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "\n",
    "def create_geo_time_key(timestamp, lat, lng):\n",
    "    # geohash映射\n",
    "    geohash = Geohash.encode(lat, lng, precision=5)\n",
    "    # 时间round至5分钟整数（floor方式）\n",
    "    tm = datetime.datetime.utcfromtimestamp(timestamp)\n",
    "    tm = tm - datetime.timedelta(minutes=tm.minute % 5, seconds=tm.second, microseconds=tm.microsecond)\n",
    "    ts_5mins = str(time.mktime(tm.timetuple()))\n",
    "    # 组合key (时间+空间)\n",
    "    key = geohash + \"_\" + ts_5mins[:-2]\n",
    "    return key\n",
    "    #return Row(service_order_id=service_order_id, ts=order_create_time, lat=expect_start_latitude, lng=expect_start_longitude, avg_can_dispatch=avg_can_dispatch, key=key)\n",
    "\n",
    "geo_time_key_udf = udf(create_geo_time_key, StringType())\n",
    "    \n",
    "\n",
    "supply_demand_df.withColumn(\"geo_time_key\", geo_time_key_udf(supply_demand_df['order_create_time'], supply_demand_df['expect_start_latitude'], supply_demand_df['expect_start_longitude'])).createOrReplaceTempView(\"v_geo_time\")\n",
    "\n",
    "spark.sql(\"select * from v_geo_time\").limit(3).toPandas().head()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 5-1-2 按时间+空间，聚合数据下单数量和周边司机数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1289903\n",
      "+-------------------+----------+------------------+\n",
      "|   service_order_id|abs_demand|   relative_supply|\n",
      "+-------------------+----------+------------------+\n",
      "|6409807271155392452|         1|               6.0|\n",
      "|6408007898860058120|         2|              null|\n",
      "|6408007537411718059|         2|              null|\n",
      "|6409477777729598208|         1|              null|\n",
      "|6411287273915704821|         2|10.333333333333334|\n",
      "+-------------------+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql = \"\"\"\n",
    "select \n",
    "a.service_order_id,\n",
    "b.abs_demand,\n",
    "b.relative_supply\n",
    "from v_geo_time a left join (\n",
    "    select\n",
    "    geo_time_key,\n",
    "    count(1) as abs_demand,\n",
    "    mean(pre_availableDriverNum) / (count(1) + 1) as relative_supply\n",
    "    from v_geo_time\n",
    "    group by geo_time_key\n",
    ") b\n",
    "on a.geo_time_key = b.geo_time_key\n",
    "\"\"\"\n",
    "\n",
    "supply_demand_df = spark.sql(sql)\n",
    "\n",
    "print supply_demand_df.count()\n",
    "\n",
    "supply_demand_df.limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-2 用户加价承受度简单画像\n",
    "1. 总体加价接受率：sum(加价且成单) / sum(加价订单)，float，建议分箱\n",
    "2. 平均成功加价倍率：avg(加价且成单的倍率)\n",
    "3. 25%成功加价倍率（近似下界）：percentile_approx(0.25)\n",
    "4. 50%成功加价倍率（近似均值）：percentile_approx(0.25)\n",
    "5. 75%成功加价倍率（近似上界）：percentile_approx(0.25)\n",
    "6. 拒绝所有加价flag：1 -> 拒绝所有加价, 0 -> 接受过任意加价，2 -> 无记录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_count:  165062\n",
      "merged count:  165062\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_order_count</th>\n",
       "      <th>user_bidding_count</th>\n",
       "      <th>user_bidding_ok_count</th>\n",
       "      <th>user_bidding_accept_flag</th>\n",
       "      <th>user_bidding_accept_pct</th>\n",
       "      <th>user_accept_avg_bidding_rate</th>\n",
       "      <th>user_accept_25pct_bidding_rate</th>\n",
       "      <th>user_accept_50pct_bidding_rate</th>\n",
       "      <th>user_accept_75pct_bidding_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>62809</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>198084</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>206344</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>209755</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>277315</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  user_order_count  user_bidding_count  user_bidding_ok_count  \\\n",
       "0    62809                 3                   0                      0   \n",
       "1   198084                 6                   1                      0   \n",
       "2   206344                 4                   3                      2   \n",
       "3   209755                10                   5                      1   \n",
       "4   277315                 8                   4                      4   \n",
       "\n",
       "   user_bidding_accept_flag  user_bidding_accept_pct  \\\n",
       "0                        -1                -1.000000   \n",
       "1                         0                 0.000000   \n",
       "2                         1                 0.666667   \n",
       "3                         1                 0.200000   \n",
       "4                         2                 1.000000   \n",
       "\n",
       "   user_accept_avg_bidding_rate  user_accept_25pct_bidding_rate  \\\n",
       "0                        -1.000                            -1.0   \n",
       "1                        -1.000                            -1.0   \n",
       "2                         0.650                             0.6   \n",
       "3                         0.200                             0.2   \n",
       "4                         0.275                             0.1   \n",
       "\n",
       "   user_accept_50pct_bidding_rate  user_accept_75pct_bidding_rate  \n",
       "0                            -1.0                           -1.00  \n",
       "1                            -1.0                           -1.00  \n",
       "2                             0.6                            0.65  \n",
       "3                             0.2                            0.20  \n",
       "4                             0.1                            0.30  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql =\"\"\"\n",
    "select \n",
    "a.user_id,\n",
    "a.user_order_count,\n",
    "a.user_bidding_count,\n",
    "a.user_bidding_ok_count,\n",
    "a.user_bidding_accept_flag,\n",
    "case when a.user_bidding_accept_pct is null then -1.0 else a.user_bidding_accept_pct end as user_bidding_accept_pct,\n",
    "case when b.user_accept_avg_bidding_rate is null then -1.0 else b.user_accept_avg_bidding_rate end as user_accept_avg_bidding_rate,\n",
    "case when b.user_accept_25pct_bidding_rate is null then -1.0 else b.user_accept_25pct_bidding_rate end as user_accept_25pct_bidding_rate,\n",
    "case when b.user_accept_50pct_bidding_rate is null then -1.0 else b.user_accept_50pct_bidding_rate end as user_accept_50pct_bidding_rate,\n",
    "case when b.user_accept_75pct_bidding_rate is null then -1.0 else b.user_accept_75pct_bidding_rate end as user_accept_75pct_bidding_rate\n",
    "from (\n",
    "    -- 用户总体订单情况\n",
    "    select \n",
    "    user_id, \n",
    "    sum(user_order_num) as user_order_count,\n",
    "    sum(is_bidding) as user_bidding_count,\n",
    "    sum(is_bidding_ok) as user_bidding_ok_count,\n",
    "    sum(is_bidding_ok) / sum(is_bidding) as user_bidding_accept_pct, -- 用户接受了的加价订单的占比\n",
    "    case\n",
    "        when sum(is_bidding)=0 then -1 -- 没有加价记录\n",
    "        when sum(is_bidding)>0 and sum(is_bidding_ok)=0 then 0 -- 拒绝了所有加价\n",
    "        when sum(is_bidding)>0 and sum(is_bidding_ok)>0 and sum(is_bidding)>sum(is_bidding_ok) then 1 -- 接受了部分加价\n",
    "        when sum(is_bidding)>0 and sum(is_bidding_ok)>0 and sum(is_bidding)=sum(is_bidding_ok) then 2 -- 接受了所有加价\n",
    "        else -2 -- 未知的case\n",
    "    end as user_bidding_accept_flag\n",
    "    from (\n",
    "        select \n",
    "        user_id, \n",
    "        1 as user_order_num,\n",
    "        case when final_driver_bidding_rate>0 or final_max_bidding_rate>0 then 1 else 0 end as is_bidding,\n",
    "        case when final_decision_driver_id>0 and final_driver_bidding_rate>0 then 1 else 0 end as is_bidding_ok\n",
    "        from `parquet`.`{path}`\n",
    "    ) tmp_a\n",
    "    group by user_id\n",
    ") a \n",
    "left join (\n",
    "    -- 用户加价成功的倍率分布\n",
    "    select \n",
    "    user_id,\n",
    "    avg(final_driver_bidding_rate) as user_accept_avg_bidding_rate,  -- 接受的平均加价倍率\n",
    "    percentile_approx(final_driver_bidding_rate, 0.25) as user_accept_25pct_bidding_rate, -- 25%接受加价倍率\n",
    "    percentile_approx(final_driver_bidding_rate, 0.50) as user_accept_50pct_bidding_rate, -- 50%（中位数）接受加价倍率\n",
    "    percentile_approx(final_driver_bidding_rate, 0.75) as user_accept_75pct_bidding_rate -- 75% 接受加价倍率\n",
    "    from (\n",
    "        select \n",
    "        user_id, \n",
    "        cast(final_driver_bidding_rate as float) as final_driver_bidding_rate\n",
    "        from `parquet`.`{path}`\n",
    "        where final_driver_bidding_rate>0 and final_decision_driver_id>0 -- 加价且撮合成功的订单\n",
    "    ) tmp_b\n",
    "    group by user_id\n",
    ") b on a.user_id=b.user_id\n",
    "\"\"\".format(path=order_allInOne_path)\n",
    "\n",
    "# print sql\n",
    "\n",
    "user_profile_df = spark.sql(sql)\n",
    "\n",
    "print \"user_count: \", spark.sql(\"select count(1) from `parquet`.`{path}` group by user_id\".format(path=order_allInOne_path)).count()\n",
    "\n",
    "print \"merged count: \", user_profile_df.count()\n",
    "\n",
    "user_profile_df.limit(5).toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5-3 地块加价成功比率\n",
    ">  按5位geohash映射，做区域内统计\n",
    "1. 区域内订单量（ ~ 是否热门区域）\n",
    "2. 区域内加价订单量\n",
    "3. 区域内加价成单量\n",
    "1. 区域内总体加价占比：count(加价订单) / count(order_id)2. 区域内加价成单占比：count(加价且成单) / count(加价)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1289903\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>service_order_id</th>\n",
       "      <th>expect_start_latitude</th>\n",
       "      <th>expect_start_longitude</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>hour_of_day</th>\n",
       "      <th>order_num</th>\n",
       "      <th>is_bidding</th>\n",
       "      <th>is_bidding_ok</th>\n",
       "      <th>expect_geohash</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6407619001781635345</td>\n",
       "      <td>39.873821</td>\n",
       "      <td>116.367985</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>wx4fb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6408477982352736592</td>\n",
       "      <td>39.964475</td>\n",
       "      <td>116.454194</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>wx4g3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6409239905257511846</td>\n",
       "      <td>40.099064</td>\n",
       "      <td>116.542989</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>wx4uh</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      service_order_id  expect_start_latitude  expect_start_longitude  \\\n",
       "0  6407619001781635345              39.873821              116.367985   \n",
       "1  6408477982352736592              39.964475              116.454194   \n",
       "2  6409239905257511846              40.099064              116.542989   \n",
       "\n",
       "   day_of_week  hour_of_day  order_num  is_bidding  is_bidding_ok  \\\n",
       "0            2           13          1           0              0   \n",
       "1            4           21          1           1              0   \n",
       "2            6           23          1           1              1   \n",
       "\n",
       "  expect_geohash  \n",
       "0          wx4fb  \n",
       "1          wx4g3  \n",
       "2          wx4uh  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql =\"\"\"\n",
    "select \n",
    "service_order_id,\n",
    "expect_start_latitude,\n",
    "expect_start_longitude,\n",
    "-- 提取星期几\n",
    "case date_format(cast(start_time as timestamp), 'EEEE')\n",
    "    WHEN 'Monday' THEN 1\n",
    "    WHEN 'Tuesday' THEN 2\n",
    "    WHEN 'Wednesday' THEN 3\n",
    "    WHEN 'Thursday' THEN 4\n",
    "    WHEN 'Friday' THEN 5\n",
    "    WHEN 'Saturday' THEN 6\n",
    "    WHEN 'Sunday' THEN 7\n",
    "    ELSE null\n",
    "END as day_of_week,\n",
    "-- 提取小时\n",
    "cast(date_format(cast(start_time as timestamp), 'HH') as int) as hour_of_day,\n",
    "1 as order_num,\n",
    "case when final_driver_bidding_rate>0 or final_max_bidding_rate>0 then 1 else 0 end as is_bidding,\n",
    "case when final_decision_driver_id>0 and final_driver_bidding_rate>0 then 1 else 0 end as is_bidding_ok\n",
    "from `parquet`.`{path}`\n",
    "\"\"\".format(path=order_allInOne_path)\n",
    "\n",
    "# print sql\n",
    "\n",
    "geo_stat_df = spark.sql(sql)\n",
    "\n",
    "print geo_stat_df.count()\n",
    "\n",
    "import Geohash\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "geohash5_udf = udf(lambda lat, lng: Geohash.encode(lat, lng, precision=5), StringType())\n",
    "    \n",
    "\n",
    "geo_stat_df.withColumn(\"expect_geohash\", geohash5_udf(geo_stat_df['expect_start_latitude'], geo_stat_df['expect_start_longitude'])).createOrReplaceTempView(\"v_geo\")\n",
    "\n",
    "spark.sql(\"select * from v_geo\").limit(3).toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 按geohash 5位聚合区域内的订单量、加价订单、加价成单\n",
    "\n",
    "TODO：按星期、小时映射为map，再做细化的连接 (目前数据稀疏，暂不这么弄)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------+-----------------+--------------------+-------------------+-------------------+\n",
      "|expect_geohash|geo_order_count|geo_bidding_count|geo_bidding_ok_count|geo_bidding_pct    |geo_bidding_ok_pct |\n",
      "+--------------+---------------+-----------------+--------------------+-------------------+-------------------+\n",
      "|wx4dv         |10375          |1923             |598                 |0.18534939759036145|0.3109724388975559 |\n",
      "|wx49s         |1              |0                |0                   |0.0                |-1.0               |\n",
      "|wx4u9         |356            |33               |17                  |0.09269662921348315|0.5151515151515151 |\n",
      "|wx4gd         |38966          |8042             |2659                |0.2063850536365036 |0.33063914449142007|\n",
      "|wx49q         |122            |15               |3                   |0.12295081967213115|0.2                |\n",
      "+--------------+---------------+-----------------+--------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql = \"\"\"\n",
    "select \n",
    "expect_geohash,\n",
    "geo_order_count,\n",
    "geo_bidding_count,\n",
    "geo_bidding_ok_count,\n",
    "case when geo_bidding_pct is null then -1 else geo_bidding_pct end  as geo_bidding_pct,\n",
    "case when geo_bidding_ok_pct is null then -1 else geo_bidding_ok_pct end as geo_bidding_ok_pct\n",
    "from (\n",
    "    select\n",
    "    expect_geohash,\n",
    "    sum(order_num) as geo_order_count,\n",
    "    sum(is_bidding) as geo_bidding_count,\n",
    "    sum(is_bidding_ok) as geo_bidding_ok_count,\n",
    "    sum(is_bidding) / sum(order_num) as geo_bidding_pct,\n",
    "    sum(is_bidding_ok) / sum(is_bidding) as geo_bidding_ok_pct\n",
    "    from v_geo\n",
    "    group by expect_geohash\n",
    ") tmp\n",
    "\"\"\"\n",
    "\n",
    "geo_stat_df = spark.sql(sql)\n",
    "\n",
    "geo_stat_df.limit(5).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-4  计算出的特征合并至订单基准数据\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order: 1289903\n",
      "left join SupplyDemand: 1289903\n",
      "left join UserProfile: 1289903\n",
      "left join GeoStatistics: 1289903\n",
      "filter no match:"
     ]
    }
   ],
   "source": [
    "orderDF = spark.sql(\"select * from `parquet`.`{path}`\".format(path=order_allInOne_path))\n",
    "\n",
    "print \"order:\", orderDF.count()\n",
    "\n",
    "# 供需数据\n",
    "dumpDF = orderDF.join(supply_demand_df, orderDF.service_order_id == supply_demand_df.service_order_id, \"left\").drop(supply_demand_df.service_order_id)\n",
    "print \"left join SupplyDemand:\", dumpDF.count()\n",
    "\n",
    "# 用户画像数据\n",
    "dumpDF = dumpDF.join(user_profile_df, dumpDF.user_id == user_profile_df.user_id, \"left\").drop(user_profile_df.user_id)\n",
    "print \"left join UserProfile:\", dumpDF.count()\n",
    "\n",
    "# 上车位置统计指标\n",
    "dumpDF = dumpDF.withColumn(\"expect_geohash\", geohash5_udf(dumpDF['expect_start_latitude'], dumpDF['expect_start_longitude']))\n",
    "dumpDF = dumpDF.join(geo_stat_df, dumpDF.expect_geohash == geo_stat_df.expect_geohash, \"left\").drop(geo_stat_df.expect_geohash)\n",
    "print \"left join GeoStatistics:\", dumpDF.count()\n",
    "\n",
    "dumpDF = dumpDF.filter(\"avg_can_dispatch>=0 and abs_demand>=0 and relative_supply>=0\")\n",
    "print \"filter no match:\", dumpDF.count()\n",
    "\n",
    "dumpDF.createOrReplaceTempView(\"v_training\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select * from v_training\").limit(3).toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5-4 导出清洗完成的数据到hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 最终加价倍率（来自加价日志，用户不一定接受）\n",
    "bidding_rate_gt0 = \"end_max_bidding_rate>0\"\n",
    "bidding_rate_eq0 = \"(end_max_bidding_rate=0 or end_max_bidding_rate is null)\"\n",
    "\n",
    "# 最终决策情况\n",
    "decision_ok = \"final_decision_driver_id>0\"\n",
    "decision_failed = \"(final_decision_driver_id=0 or final_decision_driver_id is null)\"\n",
    "\n",
    "# 是否有司机接单\n",
    "accept_ok = \"first_accept_time>0\"\n",
    "accept_failed = \"(first_accept_time>0 is null or first_accept_time=0)\"\n",
    "\n",
    "# 最终决策成功的加价倍率\n",
    "bidding_decision_ok = \"final_driver_bidding_rate>0\"\n",
    "\n",
    "spark.sql(\"drop table if exists tmp.bidding_training\")\n",
    "\n",
    "sql = \"\"\"\n",
    "create table tmp.bidding_training stored as orc as\n",
    "select \n",
    "*,\n",
    "case \n",
    "    -- 决策成功，取决策倍率\n",
    "    when final_decision_driver_id >0 then final_driver_bidding_rate\n",
    "    -- 决策不成功，触发了加价, 取加价系统末轮倍率\n",
    "    when {decision_failed} and final_max_bidding_rate>0 then end_max_bidding_rate\n",
    "    -- 决策不成功，触发了加价，但该加价倍率没有派发（用户看到就关掉了）\n",
    "    when {decision_failed} and end_max_bidding_rate>0 then end_max_bidding_rate\n",
    "    else 0\n",
    "end as bidding_rate_adjust, -- 末轮加价倍率，根据决策情况区分\n",
    "case \n",
    "    when {decision_ok} and {bidding_decision_ok} then 'bidding_decision_ok'\n",
    "    when {bidding_rate_gt0} and {decision_failed} then 'bidding_decision_failed'\n",
    "    when {bidding_rate_eq0} and {decision_ok} then 'no_bidding_decision_ok'\n",
    "    when {bidding_rate_eq0} and {decision_failed} then 'no_bidding_decision_failed'\n",
    "    else 'unkown'\n",
    "end as label\n",
    "from v_training\n",
    "\"\"\".format(decision_ok=decision_ok, decision_failed=decision_failed, bidding_decision_ok=bidding_decision_ok,\n",
    "           bidding_rate_gt0=bidding_rate_gt0, bidding_rate_eq0=bidding_rate_eq0)\n",
    "\n",
    "# print sql\n",
    "\n",
    "spark.sql(sql)\n",
    "\n",
    "print \"dump: \", spark.sql(\"select service_order_id from tmp.bidding_training\").count()\n",
    "\n",
    "print \"dump complete\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
